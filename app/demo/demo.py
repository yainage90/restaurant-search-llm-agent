"""
ë§›ì§‘ ê²€ìƒ‰ LLM ì—ì´ì „íŠ¸ì˜ í†µí•© ì¸í„°í˜ì´ìŠ¤
ì‚¬ìš©ììš© ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ì™€ ì–´ë“œë¯¼ ëŒ€ì‹œë³´ë“œë¥¼ í•˜ë‚˜ë¡œ í†µí•©í•œ Gradio ì•±
"""

import gradio as gr
from dotenv import load_dotenv
from typing import Optional
from app.retrieve.search import search, search_restaurants
from app.generation.generation import generate
from app.retrieve.relevance import grade_relevance
from app.demo.config import config, ui_messages
from app.demo.session import SessionManager
from app.demo.utils import (
    handle_exceptions, 
    parse_json_response, 
    safe_format_json, 
    calculate_keyword_similarity,
    create_session_id,
    generate_decision
)
from app.demo.ui_components import (
    create_chat_interface,
    create_admin_dashboard
)

load_dotenv()


# ì „ì—­ ì„¸ì…˜ ë§¤ë‹ˆì €
session_manager = SessionManager()


@handle_exceptions(
    default_return={
        "need_search": False, 
        "reason": ui_messages.search_decision_error
    }
)
def should_perform_new_search(
    current_message: str, 
    search_history: list[dict], 
    chat_history: list[list[str]]
) -> dict[str, str | bool]:
    """
    ìƒˆë¡œìš´ ê²€ìƒ‰ì´ í•„ìš”í•œì§€ LLMìœ¼ë¡œ íŒë‹¨í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        current_message: í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€
        search_history: ì´ì „ ê²€ìƒ‰ ê¸°ë¡
        chat_history: ëŒ€í™” ê¸°ë¡
        
    Returns:
        dict: ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨ ê²°ê³¼
    """
    # ê²€ìƒ‰ ê¸°ë¡ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    search_history_text = ""
    if search_history:
        search_history_text = "ì´ì „ ê²€ìƒ‰ ê¸°ë¡:\n"
        recent_searches = search_history[-config.max_search_history:]
        for i, search in enumerate(recent_searches, 1):
            timestamp = search['timestamp'].strftime('%H:%M')
            search_history_text += f"{i}. \"{search['query']}\" (ê²€ìƒ‰ì‹œê°„: {timestamp})\n"
    
    # ëŒ€í™” ê¸°ë¡ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    chat_history_text = ""
    if chat_history:
        chat_history_text = "ìµœê·¼ ëŒ€í™”:\n"
        recent_chat = chat_history[-config.max_chat_history:]
        for user_msg, bot_msg in recent_chat:
            chat_history_text += f"ì‚¬ìš©ì: {user_msg}\n"
            chat_history_text += f"ì–´ì‹œìŠ¤í„´íŠ¸: {bot_msg}\n"
    
    prompt = f"""ì‚¬ìš©ìì˜ í˜„ì¬ ë©”ì‹œì§€ë¥¼ ë¶„ì„í•˜ì—¬ ìƒˆë¡œìš´ ë§›ì§‘ ê²€ìƒ‰ì´ í•„ìš”í•œì§€ íŒë‹¨í•´ì£¼ì„¸ìš”.

í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€: "{current_message}"

{search_history_text}

{chat_history_text}

ë‹¤ìŒê³¼ ê°™ì€ ê²½ìš° ìƒˆë¡œìš´ ê²€ìƒ‰ì´ í•„ìš”í•©ë‹ˆë‹¤:
1. ì™„ì „íˆ ë‹¤ë¥¸ ì§€ì—­ì´ë‚˜ ìŒì‹ ì¢…ë¥˜ë¥¼ ìš”ì²­í•˜ëŠ” ê²½ìš°
2. ê¸°ì¡´ ê²€ìƒ‰ ê²°ê³¼ì— ë§Œì¡±í•˜ì§€ ëª»í•˜ê³  ë‹¤ë¥¸ ì˜µì…˜ì„ ì›í•˜ëŠ” ê²½ìš°
3. ì´ì „ ê²€ìƒ‰ê³¼ ì „í˜€ ê´€ë ¨ ì—†ëŠ” ìƒˆë¡œìš´ ë§›ì§‘ ì •ë³´ë¥¼ ìš”ì²­í•˜ëŠ” ê²½ìš°
4. ê²€ìƒ‰ ê¸°ë¡ì´ ì—†ëŠ” ìƒíƒœì—ì„œ íŠ¹ì • ì‹ë‹¹ëª…ì´ë‚˜ êµ¬ì²´ì ì¸ ë§›ì§‘ ì •ë³´ë¥¼ ìš”ì²­í•˜ëŠ” ê²½ìš°

ë‹¤ìŒê³¼ ê°™ì€ ê²½ìš°ëŠ” ê¸°ì¡´ ê²€ìƒ‰ ê²°ê³¼ë¡œ ë‹µë³€ ê°€ëŠ¥í•©ë‹ˆë‹¤:
1. ê¸°ì¡´ ê²€ìƒ‰ ê¸°ë¡ì´ ìˆê³  ê·¸ ê²€ìƒ‰ ê²°ê³¼ì— í¬í•¨ëœ íŠ¹ì • ì‹ë‹¹ì— ëŒ€í•œ ì¶”ê°€ ì§ˆë¬¸ (ë©”ë‰´, ì˜ì—…ì‹œê°„, ê°€ê²©, ìœ„ì¹˜, ì˜ˆì•½ ë°©ë²• ë“±)
2. ê¸°ì¡´ ê²€ìƒ‰ ê¸°ë¡ì´ ìˆê³  ê·¸ ì¶”ì²œ ë§›ì§‘ ì¤‘ ì„ íƒì´ë‚˜ ë¹„êµë¥¼ ìš”ì²­í•˜ëŠ” ê²½ìš°  
3. ì¼ë°˜ì ì¸ ëŒ€í™”ë‚˜ ê°ì‚¬ ì¸ì‚¬
4. ê¸°ì¡´ ê²€ìƒ‰ ê¸°ë¡ì´ ìˆê³  ê·¸ ê²€ìƒ‰ ê²°ê³¼ ë‚´ì—ì„œ ë‹µë³€í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸


ì‘ë‹µ í˜•ì‹:
{{
    "need_search": true/false,
    "reason": "íŒë‹¨ ê·¼ê±°ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ"
}}"""

    response = generate_decision(prompt)
    parsed_result = parse_json_response(response)
    
    if "error" in parsed_result:
        return {
            "need_search": False,
            "reason": parsed_result["error"]
        }
    
    return {
        "need_search": parsed_result.get("need_search", False),
        "reason": parsed_result.get("reason", "íŒë‹¨ ë¶ˆê°€")
    }


def is_similar_query(
    new_query: str, 
    search_history: list[dict], 
    similarity_threshold: Optional[float] = None
) -> bool:
    """
    ìƒˆ ì¿¼ë¦¬ê°€ ê¸°ì¡´ ê²€ìƒ‰ê³¼ ìœ ì‚¬í•œì§€ í™•ì¸
    
    Args:
        new_query: ìƒˆë¡œìš´ ê²€ìƒ‰ ì¿¼ë¦¬
        search_history: ê²€ìƒ‰ ê¸°ë¡
        similarity_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’
        
    Returns:
        bool: ìœ ì‚¬í•œ ê²€ìƒ‰ì´ ìˆìœ¼ë©´ True
    """
    if not search_history:
        return False
    
    if similarity_threshold is None:
        similarity_threshold = config.similarity_threshold
    
    recent_searches = search_history[-config.max_search_history:]
    
    for search in recent_searches:
        similarity = calculate_keyword_similarity(new_query, search['query'])
        if similarity >= similarity_threshold:
            return True
    
    return False

def chat_fn(
    message: str, 
    history: list[list[str]], 
    request: Optional[gr.Request] = None
) -> str:
    """
    ì±„íŒ… ì²˜ë¦¬ í•¨ìˆ˜ - ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰ ë¡œì§ì„ í¬í•¨í•œ ì„¸ì…˜ ê¸°ë°˜ ìƒíƒœ ê´€ë¦¬
    
    Args:
        message: ì‚¬ìš©ìì˜ í˜„ì¬ ë©”ì‹œì§€
        history: ì´ì „ ëŒ€í™” ê¸°ë¡
        request: Gradio ìš”ì²­ ê°ì²´ (ì„¸ì…˜ ID ì¶”ì¶œìš©)
        
    Returns:
        str: ì‘ë‹µ ë©”ì‹œì§€
    """
    # ì„¸ì…˜ ID ì¶”ì¶œ ë° ì„¸ì…˜ ê´€ë¦¬
    session_id = None
    if request:
        session_id = create_session_id(request.client.host)
    
    session = session_manager.get_or_create_session(session_id)
    session_manager.cleanup_old_sessions()

    if not history or not session.has_context():
        # ì²« ëŒ€í™”: ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨ í›„ ì²˜ë¦¬
        return _handle_first_chat(message, session)
    else:
        # í›„ì† ëŒ€í™”: ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰ ë¡œì§ ì ìš©
        return _handle_follow_up_chat(message, history, session)


def _handle_first_chat(message: str, session) -> str:
    """ì²« ë²ˆì§¸ ëŒ€í™” ì²˜ë¦¬ - ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨"""
    # ë¹ˆ ê¸°ë¡ìœ¼ë¡œ ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨
    search_decision = should_perform_new_search(message, [], [])
    print(f'[Session {session.session_id}] ì²« ëŒ€í™” ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨: {search_decision}')
    
    if search_decision["need_search"]:
        # ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš°
        return _perform_new_search(message, session)
    else:
        # ê²€ìƒ‰ì´ ë¶ˆí•„ìš”í•œ ê²½ìš° - ì¼ë°˜ì ì¸ ì‘ë‹µ ìƒì„±
        try:
            bot_response = generate(query="", context=f"ì‚¬ìš©ì ë©”ì‹œì§€: {message}")
            print(f'[Session {session.session_id}] ì²« ëŒ€í™” - ê²€ìƒ‰ ì—†ì´ ì‘ë‹µ')
            return bot_response
        except Exception as e:
            print(f'[Session {session.session_id}] ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}')
            return f"{ui_messages.error_prefix}{str(e)}"


def _handle_follow_up_chat(message: str, history: list[list[str]], session) -> str:
    """í›„ì† ëŒ€í™” ì²˜ë¦¬"""
    # ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨
    search_decision = should_perform_new_search(message, session.search_history, history)
    print(f'[Session {session.session_id}] ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨: {search_decision}')
    
    if search_decision["need_search"]:
        # ì¤‘ë³µ ê²€ìƒ‰ ë°©ì§€
        if is_similar_query(message, session.search_history):
            print(f'[Session {session.session_id}] ìœ ì‚¬í•œ ê²€ìƒ‰ìœ¼ë¡œ íŒë‹¨, ê¸°ì¡´ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©')
        else:
            # ìƒˆë¡œìš´ ê²€ìƒ‰ ìˆ˜í–‰
            return _perform_new_search(message, session)
    
    # ê¸°ì¡´ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‘ë‹µ ìƒì„±
    return _generate_context_response(message, history, session)


def _perform_new_search(query: str, session) -> str:
    """ìƒˆë¡œìš´ ê²€ìƒ‰ ìˆ˜í–‰"""
    try:
        print(f'[Session {session.session_id}] ìƒˆë¡œìš´ ê²€ìƒ‰ ìˆ˜í–‰ - Query: {query}')
        restaurant_context = search(query)
        session.update_context(query, restaurant_context)
        
        bot_response = generate(query, restaurant_context)
        return bot_response
    except Exception as e:
        print(f'[Session {session.session_id}] ìƒˆ ê²€ìƒ‰ ì˜¤ë¥˜: {e}')
        return f"{ui_messages.error_prefix}{str(e)}"


def _generate_context_response(message: str, history: list[list[str]], session) -> str:
    """ê¸°ì¡´ ì»¨í…ìŠ¤íŠ¸ë¥¼ í™œìš©í•œ ì‘ë‹µ ìƒì„±"""
    try:
        chat_history_prompt = ""
        for user_msg, bot_msg in history:
            chat_history_prompt += f"ì‚¬ìš©ì: {user_msg}\n"
            chat_history_prompt += f"ì–´ì‹œìŠ¤í„´íŠ¸: {bot_msg}\n"
        
        chat_history_prompt += f"ì‚¬ìš©ì: {message}\n"
        stored_context = session.get_context()
        
        response_text = generate(
            query="",
            context=f"ì°¸ê³  ì •ë³´: {stored_context}\n\n---ëŒ€í™” ê¸°ë¡---\n{chat_history_prompt}"
        )
        
        print(f'[Session {session.session_id}] ê¸°ì¡´ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‘ë‹µ - ê²€ìƒ‰ íšŸìˆ˜: {session.get_search_count()}')
        return response_text
    except Exception as e:
        print(f'[Session {session.session_id}] ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}')
        return f"{ui_messages.error_prefix}{str(e)}"


@handle_exceptions(default_return="")
def test_nlu_module(query: str) -> str:
    """NLU ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ - ì˜ë„ ë¶„ë¥˜ ë° ì—”í‹°í‹° ì¶”ì¶œ"""
    from ..retrieve.nlu import classify_intent_and_extract_entities
    result = classify_intent_and_extract_entities(query)
    return safe_format_json(result)


@handle_exceptions(default_return=("", "", ""))
def test_search_module(query: str) -> tuple[str, str, str]:
    """ê²€ìƒ‰ ëª¨ë“ˆ ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
    from app.retrieve.nlu import classify_intent_and_extract_entities
    from app.retrieve.hybrid_search import (
        hybrid_search, 
        build_bm25_query, 
        build_vector_query
    )
    from app.retrieve.embeddings import get_query_embedding
    from app.retrieve.search import search_restaurants_by_intent, filter_by_relevance
    
    # 1. NLU ë¶„ì„ (ìƒˆë¡œìš´ suggested_queries í¬í•¨)
    nlu_result = classify_intent_and_extract_entities(query)
    intent = nlu_result.get("intent", "search")
    entities = nlu_result.get("entities", {})
    suggested_queries = nlu_result.get("suggested_queries", [query])
    
    # 2. ê²€ìƒ‰ í¬ê¸° ê²°ì •
    if intent == "search":
        size = 5
    elif intent == "compare":
        size = 8
    elif intent == "information":
        size = 3
    else:
        size = 5
    
    # 3. Elasticsearch ì¿¼ë¦¬ ìƒì„± (í‘œì‹œìš© - ì²« ë²ˆì§¸ suggested_query ì‚¬ìš©)
    display_query = suggested_queries[0] if suggested_queries else query
    search_size = max(50, size * 10)
    
    # BM25 ì¿¼ë¦¬ ìƒì„±
    bm25_query = build_bm25_query(display_query, entities, search_size)
    
    # ë²¡í„° ì¿¼ë¦¬ ìƒì„±
    query_embedding = get_query_embedding([display_query])
    vector_query = build_vector_query(query_embedding, entities, search_size)
    
    # ë²¡í„° ìš”ì•½ìœ¼ë¡œ ëŒ€ì²´ (ì¶œë ¥ìš©)
    import copy
    display_vector_query = copy.deepcopy(vector_query)
    embedding_summary = f"[{len(query_embedding)}ì°¨ì› ë²¡í„°]" if query_embedding else "ì„ë² ë”© ì—†ìŒ"
    if "knn" in display_vector_query:
        display_vector_query["knn"]["query_vector"] = embedding_summary
    
    # 4. ì—°ê´€ë„ í•„í„°ë§ ì´ì „ ê²€ìƒ‰ ê²°ê³¼ (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ë§Œ)
    pre_filter_results = hybrid_search(suggested_queries, entities, intent, size)
    
    # 5. ì—°ê´€ë„ í•„í„°ë§ í›„ ìµœì¢… ê²€ìƒ‰ ê²°ê³¼
    final_results = filter_by_relevance(query, pre_filter_results) if pre_filter_results else []
    
    # ì¶œë ¥ í¬ë§·íŒ…
    nlu_str = safe_format_json(nlu_result)
    
    # Elasticsearch ì¿¼ë¦¬ ì •ë³´
    es_queries = {
        "BM25_ì¿¼ë¦¬": bm25_query,
        "ë²¡í„°_ì¿¼ë¦¬": display_vector_query,
        "ê²€ìƒ‰_í¬ê¸°": search_size,
        "ê²€ìƒ‰_ì˜ë„": intent,
        "ì‚¬ìš©ëœ_ì¿¼ë¦¬ë“¤": suggested_queries
    }
    es_query_str = safe_format_json(es_queries)
    
    # ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥ (í•„í„°ë§ ì „/í›„ ëª¨ë‘ í‘œì‹œ)
    search_results_text = ""
    
    # í•„í„°ë§ ì´ì „ ê²°ê³¼
    search_results_text += "=== ì—°ê´€ë„ í•„í„°ë§ ì´ì „ ê²°ê³¼ ===\n\n"
    if pre_filter_results:
        for i, result in enumerate(pre_filter_results[:3], 1):  # ìƒìœ„ 3ê°œë§Œ
            title = result.get("title", "ì œëª© ì—†ìŒ")
            summary = result.get("summary", "N/A")
            category = result.get("category", "ì¹´í…Œê³ ë¦¬ ì—†ìŒ")
            convenience = result.get("convenience", [])
            
            # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì ìˆ˜ ì •ë³´ í‘œì‹œ
            rrf_score = result.get("rrf_score", "N/A")
            search_method = result.get("search_method", "N/A")
            bm25_rank = result.get("bm25_rank", "N/A")
            vector_rank = result.get("vector_rank", "N/A")
            
            search_results_text += f"{i}. {title}\n"
            search_results_text += f"   RRFì ìˆ˜: {rrf_score}, ë°©ë²•: {search_method}\n"
            if bm25_rank != "N/A":
                search_results_text += f"   BM25 ìˆœìœ„: {bm25_rank}\n"
            if vector_rank != "N/A":
                search_results_text += f"   ë²¡í„° ìˆœìœ„: {vector_rank}\n"
            search_results_text += f"   ìš”ì•½:\n{summary}...\n\n"
    else:
        search_results_text += "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\n\n"
    
    # í•„í„°ë§ ì´í›„ ê²°ê³¼
    search_results_text += "=== ì—°ê´€ë„ í•„í„°ë§ í›„ ìµœì¢… ê²°ê³¼ ===\n\n"
    if final_results:
        for i, result in enumerate(final_results[:3], 1):  # ìƒìœ„ 3ê°œë§Œ
            title = result.get("title", "ì œëª© ì—†ìŒ")
            summary = result.get("summary", "N/A")
            category = result.get("category", "ì¹´í…Œê³ ë¦¬ ì—†ìŒ")
            convenience = result.get("convenience", [])
            
            # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì ìˆ˜ ì •ë³´ í‘œì‹œ
            rrf_score = result.get("rrf_score", "N/A")
            search_method = result.get("search_method", "N/A")
            bm25_rank = result.get("bm25_rank", "N/A")
            vector_rank = result.get("vector_rank", "N/A")
            
            search_results_text += f"{i}. {title}\n"
            search_results_text += f"   ì¹´í…Œê³ ë¦¬: {category}\n"
            search_results_text += f"   í¸ì˜ì‚¬í•­: {convenience}\n"
            search_results_text += f"   RRFì ìˆ˜: {rrf_score}, ë°©ë²•: {search_method}\n"
            if bm25_rank != "N/A":
                search_results_text += f"   BM25 ìˆœìœ„: {bm25_rank}\n"
            if vector_rank != "N/A":
                search_results_text += f"   ë²¡í„° ìˆœìœ„: {vector_rank}\n"
            search_results_text += f"   ìš”ì•½: {summary[:200]}...\n\n"
    else:
        search_results_text += "ì—°ê´€ë„ í•„í„°ë§ìœ¼ë¡œ ì¸í•´ ëª¨ë“  ê²°ê³¼ê°€ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n"
    
    return (
        f"NLU ë¶„ì„ ê²°ê³¼:\n{nlu_str}\n\n",
        f"Elasticsearch ì¿¼ë¦¬:\n{es_query_str}",
        f"ê²€ìƒ‰ ê²°ê³¼ ë¹„êµ:\n\n{search_results_text}"
    )


@handle_exceptions(default_return="")
def get_relevance_evaluation(query: str) -> str:
    """ì—°ê´€ì„± í‰ê°€ ê²°ê³¼ë§Œ ë°˜í™˜"""
    # ì‹¤ì œ Elasticsearch ê²€ìƒ‰ ìˆ˜í–‰
    documents = search_restaurants(query)
    
    if not documents:
        return ui_messages.no_search_results
    
    # ì—°ê´€ì„± í‰ê°€ ìˆ˜í–‰
    relevance_result = grade_relevance(query, documents)
    
    # ê²°ê³¼ í¬ë§·íŒ…
    result_text = f"=== ì—°ê´€ì„± í‰ê°€ ê²°ê³¼ ===\n"
    result_text += f"ì¿¼ë¦¬: {query}\n"
    result_text += f"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(documents)}ê°œ\n\n"
    
    # ì—°ê´€ì„± í‰ê°€ ê²°ê³¼ í‘œì‹œ
    if relevance_result:
        overall_relevance = relevance_result.get("overall_relevance", "ì•Œ ìˆ˜ ì—†ìŒ")
        overall_reason = relevance_result.get("reason", "ì´ìœ  ì—†ìŒ")
        document_scores = relevance_result.get("document_scores", [])
        
        result_text += f"ì „ì²´ ì—°ê´€ì„±: {overall_relevance}\n"
        result_text += f"ì „ì²´ íŒë‹¨ ê·¼ê±°: {overall_reason}\n\n"
        
        if document_scores:
            result_text += "ê°œë³„ ë¬¸ì„œ ì—°ê´€ì„± í‰ê°€:\n"
            for score in document_scores:
                doc_id = score.get("document_id", "ì•Œ ìˆ˜ ì—†ìŒ")
                relevance = score.get("relevance", "ì•Œ ìˆ˜ ì—†ìŒ")
                reason = score.get("reason", "ì´ìœ  ì—†ìŒ")
                
                # í•´ë‹¹ ë¬¸ì„œì˜ ì œëª©ê³¼ ìš”ì•½ ê°€ì ¸ì˜¤ê¸°
                try:
                    doc_index = int(doc_id) - 1
                    if 0 <= doc_index < len(documents):
                        doc = documents[doc_index]
                        title = doc.get("title", "ì œëª© ì—†ìŒ")
                    else:
                        title = "ì œëª© ì—†ìŒ"
                except (ValueError, TypeError):
                    title = "ì œëª© ì—†ìŒ"
                
                result_text += f"\në¬¸ì„œ {doc_id}: {title}\n"
                result_text += f"   ì—°ê´€ì„±: {relevance}\n"
                result_text += f"   íŒë‹¨ ê·¼ê±°: {reason}\n"
        else:
            result_text += "ê°œë³„ ë¬¸ì„œ í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
    else:
        result_text += "ì—°ê´€ì„± í‰ê°€ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
        
    return result_text


@handle_exceptions(default_return="")
def get_search_results_summary(query: str) -> str:
    """ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ë§Œ ë°˜í™˜"""
    # ì‹¤ì œ Elasticsearch ê²€ìƒ‰ ìˆ˜í–‰
    documents = search_restaurants(query)
    
    if not documents:
        return ui_messages.no_search_results
    
    # ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ í‘œì‹œ
    result_text = f"=== ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ ===\n"
    result_text += f"ì´ {len(documents)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ë¨\n\n"
    
    for i, doc in enumerate(documents, 1):
        title = doc.get("title", "ì œëª© ì—†ìŒ")
        summary = doc.get("summary", "ìš”ì•½ ì •ë³´ ì—†ìŒ")
        search_score = doc.get("_score", "ì ìˆ˜ ì—†ìŒ")
        result_text += f"{i}. {title}\n"
        result_text += f"   ê²€ìƒ‰ ì ìˆ˜: {search_score}\n"
        result_text += f"   ìš”ì•½: {summary}\n\n"
    
    return result_text


@handle_exceptions(default_return="")
def test_full_pipeline(query: str) -> str:
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ (ì‹¤ì œ ê²€ìƒ‰ + ì—°ê´€ì„± í‰ê°€)"""
    # ì‹¤ì œ ê²€ìƒ‰ ìˆ˜í–‰
    results = search_restaurants(query)
    
    if not results:
        return ui_messages.no_search_results
    
    # ì—°ê´€ì„± í‰ê°€
    relevance_result = grade_relevance(query, results)
    
    # ê²°ê³¼ ì •ë¦¬
    pipeline_result = {
        "ê²€ìƒ‰_ê²°ê³¼_ìˆ˜": len(results),
        "ì—°ê´€ì„±_í‰ê°€": relevance_result,
        "ê²€ìƒ‰ëœ_ì—…ì²´ë“¤": [
            {
                "ì—…ì²´ëª…": doc.get("title", "N/A"),
                "ì ìˆ˜": doc.get("_score", "N/A"),
                "ìš”ì•½": doc.get("summary", "N/A")[:100] + "..." 
                       if len(doc.get("summary", "")) > 100 
                       else doc.get("summary", "N/A")
            }
            for doc in results
        ]
    }
    
    return safe_format_json(pipeline_result)


def create_interface() -> gr.Blocks:
    """í†µí•© Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
    
    with gr.Blocks(title="ë§›ì§‘ ê²€ìƒ‰ LLM ì—ì´ì „íŠ¸", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# ğŸ½ï¸ ë§›ì§‘ ê²€ìƒ‰ LLM ì—ì´ì „íŠ¸")
        
        with gr.Tabs():
            # ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
            with gr.Tab("ğŸ’¬ ì±„íŒ…"):
                create_chat_interface(chat_fn)
            
            # ì–´ë“œë¯¼ ëŒ€ì‹œë³´ë“œ
            with gr.Tab("ğŸ”§ ì–´ë“œë¯¼ ëŒ€ì‹œë³´ë“œ"):
                create_admin_dashboard(
                    test_nlu_module,
                    test_search_module,
                    get_relevance_evaluation,
                    get_search_results_summary
                )
    
    return demo


if __name__ == "__main__":
    interface = create_interface()
    interface.launch(
        server_name=config.server_name,
        server_port=config.server_port,
        share=config.share
    )